{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom collections import Counter\n\nimport random\nimport json\nimport re\nfrom sklearn.manifold import TSNE\nfrom scipy import spatial\nimport matplotlib.pyplot as plt\nimport pickle\nimport copy\nfrom tqdm import tqdm\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(0)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T13:45:22.903685Z","iopub.execute_input":"2023-05-05T13:45:22.904118Z","iopub.status.idle":"2023-05-05T13:45:26.454039Z","shell.execute_reply.started":"2023-05-05T13:45:22.904082Z","shell.execute_reply":"2023-05-05T13:45:26.453075Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7e2329c073f0>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Parameters","metadata":{}},{"cell_type":"code","source":"# no of projection matrices\nk = 24\n\n# no of dimentions in embedding\ndim = 300\n\n# no of negative samples\nneg_sample_count = 5\n\n# learning rate\nlr = 0.001\n\nbatch_size = 32\n\n# 1A 2A 2B\nsubtask = \"1A\"\n\n# training test\nphase = \"training\"\n\n# datafile\ndataFilePath = f\"/kaggle/input/inlp-project/{subtask}.english.{phase}.data.txt\"\n\n# goldfile\ngoldFilePath = f\"/kaggle/input/inlp-project/{subtask}.english.{phase}.gold.txt\"\n\n# vocab\nvocabFilePath = f\"/kaggle/input/inlp-project/{subtask}.english.vocabulary.txt\"\n","metadata":{"execution":{"iopub.status.busy":"2023-05-05T13:45:26.455968Z","iopub.execute_input":"2023-05-05T13:45:26.456567Z","iopub.status.idle":"2023-05-05T13:45:26.461913Z","shell.execute_reply.started":"2023-05-05T13:45:26.456533Z","shell.execute_reply":"2023-05-05T13:45:26.460648Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Data loading and preprocessing","metadata":{}},{"cell_type":"code","source":"file = open(f\"/kaggle/input/inlp-project/hypernym-hyponym-dictionaries_{subtask}.pkl\",'rb')\nparameters = pickle.load(file)\nfile.close()\n\nvocab = parameters['vocab']\nw2i = parameters['w2i']\ni2w = parameters['i2w']\ndata = parameters['hyponyms']\ngold = parameters['hypernyms']","metadata":{"execution":{"iopub.status.busy":"2023-05-05T13:45:26.463422Z","iopub.execute_input":"2023-05-05T13:45:26.464095Z","iopub.status.idle":"2023-05-05T13:45:26.788490Z","shell.execute_reply.started":"2023-05-05T13:45:26.464065Z","shell.execute_reply":"2023-05-05T13:45:26.787569Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"print(len(data))","metadata":{"execution":{"iopub.status.busy":"2023-05-05T13:45:26.790794Z","iopub.execute_input":"2023-05-05T13:45:26.791510Z","iopub.status.idle":"2023-05-05T13:45:26.797023Z","shell.execute_reply.started":"2023-05-05T13:45:26.791474Z","shell.execute_reply":"2023-05-05T13:45:26.796015Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"1500\n","output_type":"stream"}]},{"cell_type":"code","source":"vocab_size = len(vocab)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T13:45:26.798127Z","iopub.execute_input":"2023-05-05T13:45:26.798775Z","iopub.status.idle":"2023-05-05T13:45:26.806661Z","shell.execute_reply.started":"2023-05-05T13:45:26.798744Z","shell.execute_reply":"2023-05-05T13:45:26.805704Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Supervised Learning - Model Architecture","metadata":{}},{"cell_type":"code","source":"class HHD(nn.Module):\n    def __init__(self, vocab_size, embedding_size):\n        super(HHD, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_size)\n\n        self.output = nn.Linear(k, 1)\n        \n        var = 2 / (dim + dim)\n        \n        # Initialize projection matrices using scheme from Glorot & Bengio (2008).\n        \n        self.proj_mats = torch.zeros([k, dim, dim], dtype=torch.float32).to(device)\n        # Fills self tensor with elements samples from the normal distribution parameterized by mean and std.\n        self.proj_mats.normal_(0, var)\n        # mat_data is of size k*dim*dim\n        # finally mat_data is k*dim*dim matrix ie k projection matrices, each matric is populated with random value\n        # diagonal elements will be 1+random value and other will be 0+random value and random value will range 0 and var\n        self.proj_mats += torch.cat([torch.eye(dim, ).unsqueeze(0) for _ in range(k)]).to(device)\n        self.sigmoid = nn.Sigmoid()\n        \n\n    def similarity(self,query, cand_hypernym,bs):\n        query = self.embedding(query) #1*d\n        cand_hypernymT = self.embedding(cand_hypernym) #bs*d\n        \n        #proj is of dim d*d, q is 1*d\n        qT = torch.transpose(query,0,1).to(device) # d*1\n        projT = torch.matmul(self.proj_mats,qT).to(device) #k*d*d X d*1 = k*d*1\n        projT = torch.squeeze(projT,2).to(device) #k*d\n        proj = torch.transpose(projT,0,1).to(device) #d*k\n        \n        # find similarity between query and candidate \n        cand_hypernym = torch.transpose(cand_hypernymT,0,1) #d*bs\n        simPosHyper = torch.matmul(projT,cand_hypernym).to(device) #k*d x d*bs = k*bs\n#         simPosHyper = torch.squeeze(simPosHyper,1) # k\n        simPosHyper = torch.transpose(simPosHyper,0,1) # bs*k\n        simPos = self.output(simPosHyper) # bs*1\n        simPos = self.sigmoid(simPos) #bs*1\n        \n        return simPos\n        \n\n    def forward(self, query, cand_hypernym, neg_hypernyms ):\n        # query - 255 , cand_hypernym - 255, neg_hypernyms - 255*5\n        # getting embeddings of required entities\n        query = self.embedding(query) #bs*d\n        cand_hypernymT = self.embedding(cand_hypernym) #bs*d\n        neg_hypernymsT = self.embedding(neg_hypernyms) #bs*ns*d\n        \n        query = torch.unsqueeze(query,2) # bs*d*1\n            \n        batch_proj = torch.tensor([]).to(device)\n        for i,q in enumerate(query):\n            projT = torch.matmul(self.proj_mats,q).to(device) # k*d*d X d*1 = k*d*1\n            projT = torch.squeeze(projT,2) # k*d\n            projT = projT.reshape([-1])\n            batch_proj = torch.cat((batch_proj,projT))\n        \n        batch_proj = batch_proj.reshape([-1,k,dim])\n        \n        \n        # find similarity between query and candidate \n        cand_hypernym = torch.unsqueeze(cand_hypernymT,2) #bs*d*1\n        simPos = torch.bmm(batch_proj,cand_hypernym) #bs*k*d x bs*d*1 = bs*k*1\n        simPos = torch.squeeze(simPos,2) #bs*k\n        simPosOutput = self.output(simPos) #bs*1\n        \n        \n        # a step from above\n        # find similarity between query and negative samples\n        batch_projT = torch.transpose(batch_proj,1,2) #bs*d*k\n        simNegs = torch.bmm(neg_hypernymsT,batch_projT) #bs*ns*d x bs*d*k = bs*ns*k\n        simNegsOutput = self.output(simNegs) #bs*ns*1\n        simNegsOutput = torch.squeeze(simNegsOutput,2) #bs*ns\n        \n        \n        \n        # simPos - bs*1, simNegs - bs*ns\n        return simPosOutput,simNegsOutput\n    \n        # getting embeddings of required entities\n        query = self.embedding(query)\n        cand_hypernymT = self.embedding(cand_hypernym) #1*d\n        neg_hypernymsT = self.embedding(neg_hypernyms) #ns*1*d\n        \n        #proj is of dim d*d, q is 1*d\n        qT = torch.transpose(query,0,1) # d*1\n        projT = torch.matmul(self.proj_mats,qT).to(device)\n        projT = torch.squeeze(projT,2) # k*d*d X d*1 = k*d*1\n        proj = torch.transpose(projT,0,1) #k*d\n        \n        # find similarity between query and candidate \n        cand_hypernym = torch.transpose(cand_hypernymT,0,1) #d*1\n        simPosHyper = torch.matmul(proj,cand_hypernym).to(device) #k*d x d*1 = k*1\n        simPosHyper = torch.squeeze(simPosHyper,1) # k\n        simPos = output(simPosHyper) # 1\n        \n        # find similarity between query and negative samples\n        #neg_hypernyms = torch.transpose(neg_hypernymsT,1,2) # ns*d*1\n        simNegHypersT = torch.matmul(neg_hypernymsT,projT).to(device) # ns*1*d x d*k= ns*1*k\n        simNegHypers = torch.transpose(simNegHypersT,1,2) # ns*k*1\n        simNegHypers = torch.squeeze(simNegHypers,2) # ns*k\n        simNegs = output(simNegHypers) # ns*1\n        simNegs = torch.squeeze(simNegs,1) # ns\n        \n        return simPos,simNegs\n    \n            ","metadata":{"execution":{"iopub.status.busy":"2023-05-05T13:45:29.162681Z","iopub.execute_input":"2023-05-05T13:45:29.163033Z","iopub.status.idle":"2023-05-05T13:45:29.180386Z","shell.execute_reply.started":"2023-05-05T13:45:29.163005Z","shell.execute_reply":"2023-05-05T13:45:29.179516Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Projection model","metadata":{}},{"cell_type":"code","source":"projection_model = HHD(vocab_size,dim)\nprojection_model.to(device)\nprojection_model = torch.load(\"/kaggle/input/inlp-project/HH_Projection_model_1A.pt\")\nprojection_model.eval()","metadata":{"execution":{"iopub.status.busy":"2023-05-05T13:45:31.104817Z","iopub.execute_input":"2023-05-05T13:45:31.105193Z","iopub.status.idle":"2023-05-05T13:45:38.638330Z","shell.execute_reply.started":"2023-05-05T13:45:31.105142Z","shell.execute_reply":"2023-05-05T13:45:38.637464Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"HHD(\n  (embedding): Embedding(219246, 300)\n  (output): Linear(in_features=24, out_features=1, bias=True)\n  (sigmoid): Sigmoid()\n)"},"metadata":{}}]},{"cell_type":"code","source":"'''\n    predict function will take a query, a word and will return list of \n    100 closest words according to projection learning model ie supervised learning\n'''\ndef predict_supervised(query):\n    \n    try:\n        q = torch.tensor([w2i[query]]).to(device)\n    except:\n        return \"word not found in vocab\"\n    \n    closest_hypernyms = [] \n    \n    h = torch.tensor(list(range(1,vocab_size))).to(device)\n    s = projection_model.similarity(q,h,h.shape[0]) #bs*1\n\n    for i in range(1,vocab_size):\n        closest_hypernyms.append([float(s[i-1]),vocab[i]])\n    closest_hypernyms.sort(reverse=True)\n    answer = []\n    \n    l = 100\n    if l>len(closest_hypernyms):\n        l = len(closest_hypernyms)\n    \n    for i in range(l):\n        answer.append(closest_hypernyms[i][1])\n        \n    return answer","metadata":{"execution":{"iopub.status.busy":"2023-05-05T13:45:38.640794Z","iopub.execute_input":"2023-05-05T13:45:38.641137Z","iopub.status.idle":"2023-05-05T13:45:38.648751Z","shell.execute_reply.started":"2023-05-05T13:45:38.641106Z","shell.execute_reply":"2023-05-05T13:45:38.647820Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Unsupervised Learning","metadata":{}},{"cell_type":"markdown","source":"### Compute dictionary with key as hyponym and value as list of hypernyms","metadata":{}},{"cell_type":"code","source":"def compute_hyponym_hypernyms(data,gold):\n  hyponym_classification = {}\n  hyponym_hypernyms = {}\n  for i in range(len(data)):\n    hyponym = data[i]\n    hypernyms = gold[i]\n    hyponym_hypernyms[hyponym] = hypernyms\n  return hyponym_hypernyms","metadata":{"execution":{"iopub.status.busy":"2023-05-05T13:45:38.650234Z","iopub.execute_input":"2023-05-05T13:45:38.650688Z","iopub.status.idle":"2023-05-05T13:45:38.658985Z","shell.execute_reply.started":"2023-05-05T13:45:38.650657Z","shell.execute_reply":"2023-05-05T13:45:38.658067Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"hyponym_hypernyms_eng = compute_hyponym_hypernyms(data,gold)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T13:45:38.661713Z","iopub.execute_input":"2023-05-05T13:45:38.662003Z","iopub.status.idle":"2023-05-05T13:45:38.670057Z","shell.execute_reply.started":"2023-05-05T13:45:38.661981Z","shell.execute_reply":"2023-05-05T13:45:38.669202Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Compute dictionary with key as hypernym and value as list of hyponyms ","metadata":{}},{"cell_type":"code","source":"def compute_hypernym_hyponyms(hyponym_hypernyms):\n\n  hypernym_hyponyms = {}\n  for i in hyponym_hypernyms:\n    hypernyms_list = hyponym_hypernyms[i]\n    for j in hypernyms_list:\n      if j in hypernym_hyponyms:\n        hypernym_hyponyms[j].append(i)\n      else:\n        hypernym_hyponyms[j] = [i]\n  for i in hyponym_hypernyms:\n    hypernyms_list = set(hyponym_hypernyms[i])\n    hyponym_hypernyms[i] = hypernyms_list\n  \n  return hypernym_hyponyms","metadata":{"execution":{"iopub.status.busy":"2023-05-05T13:45:38.672932Z","iopub.execute_input":"2023-05-05T13:45:38.673196Z","iopub.status.idle":"2023-05-05T13:45:38.680507Z","shell.execute_reply.started":"2023-05-05T13:45:38.673168Z","shell.execute_reply":"2023-05-05T13:45:38.679643Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"hypernym_hyponyms_eng = compute_hypernym_hyponyms(hyponym_hypernyms_eng)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T13:45:38.774480Z","iopub.execute_input":"2023-05-05T13:45:38.774766Z","iopub.status.idle":"2023-05-05T13:45:38.788681Z","shell.execute_reply.started":"2023-05-05T13:45:38.774743Z","shell.execute_reply":"2023-05-05T13:45:38.787741Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Load custom trained with negative sampling word2vec embeddings","metadata":{}},{"cell_type":"code","source":"# Word2Vec pretrained embeddings further trained with hypernym negative sampling\n\nwith open('/kaggle/input/inlp-project/hypernym-hyponym-embeddings_1A.pkl', 'rb') as f:\n    emb = pickle.load(f)\n    \nword2vec_embeddings_eng = emb","metadata":{"execution":{"iopub.status.busy":"2023-05-05T13:45:39.391938Z","iopub.execute_input":"2023-05-05T13:45:39.392311Z","iopub.status.idle":"2023-05-05T13:45:43.940884Z","shell.execute_reply.started":"2023-05-05T13:45:39.392281Z","shell.execute_reply":"2023-05-05T13:45:43.939951Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"queries_eng = data","metadata":{"execution":{"iopub.status.busy":"2023-05-05T13:45:43.943585Z","iopub.execute_input":"2023-05-05T13:45:43.944238Z","iopub.status.idle":"2023-05-05T13:45:43.948496Z","shell.execute_reply.started":"2023-05-05T13:45:43.944204Z","shell.execute_reply":"2023-05-05T13:45:43.947504Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"len(queries_eng)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T13:45:43.949960Z","iopub.execute_input":"2023-05-05T13:45:43.950373Z","iopub.status.idle":"2023-05-05T13:45:43.961308Z","shell.execute_reply.started":"2023-05-05T13:45:43.950344Z","shell.execute_reply":"2023-05-05T13:45:43.960200Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"1500"},"metadata":{}}]},{"cell_type":"markdown","source":"### Compute co-hyponyms for given set of hyponyms","metadata":{}},{"cell_type":"code","source":"Q = []\nHq = []\nco_hyponyms_query = {}\n\n# query -> hyponym\nfor query in queries_eng:                            \n  # get hypernyms for a given hyponym \n  hypernyms_query = hyponym_hypernyms_eng[query]     \n  co_hyponyms = []\n  for hypernym in hypernyms_query:                   \n    # store all hyponyms of the hypernyms derived from above in the list \"co-hyponyms\" \n    for hyponym in hypernym_hyponyms_eng[hypernym]:\n        if hyponym != query:         \n            # append co-hyponym only if it is not the original hyponym\n            co_hyponyms.append(hyponym)\n  \n  #compute set of co-hyponyms list to get unique co-hyponyms\n  co_hyponyms_set = set(co_hyponyms)        \n  co_hyponyms_freq = {}\n  # compute frequency of each co-hyponym of a given hyponym and store in co_hyponyms_query\n  for co_hyponym in co_hyponyms_set:\n    freq = co_hyponyms.count(co_hyponym)\n    co_hyponyms_freq[co_hyponym] = freq\n\n  co_hyponyms_query[query] = co_hyponyms_freq","metadata":{"execution":{"iopub.status.busy":"2023-05-05T13:45:43.964180Z","iopub.execute_input":"2023-05-05T13:45:43.964610Z","iopub.status.idle":"2023-05-05T13:45:44.654127Z","shell.execute_reply.started":"2023-05-05T13:45:43.964587Z","shell.execute_reply":"2023-05-05T13:45:44.653217Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### Compute cosine similarities","metadata":{}},{"cell_type":"code","source":"from numpy.linalg import norm\n\n# cosine similarity with custom trained word2vec embeddings\ndef calculate_cosine_similarity_word2vec(a,b):\n  A = np.zeros(300)\n  B = np.zeros(300)\n  if a in word2vec_embeddings_eng and b in word2vec_embeddings_eng:\n    A = word2vec_embeddings_eng[a]\n    B = word2vec_embeddings_eng[b]\n    cosine = np.dot(A,B)/(norm(A)*norm(B))\n    return cosine\n  else:\n    return 0","metadata":{"execution":{"iopub.status.busy":"2023-05-05T13:45:44.656226Z","iopub.execute_input":"2023-05-05T13:45:44.657124Z","iopub.status.idle":"2023-05-05T13:45:44.663656Z","shell.execute_reply.started":"2023-05-05T13:45:44.657087Z","shell.execute_reply":"2023-05-05T13:45:44.662472Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# queries_eng","metadata":{"execution":{"iopub.status.busy":"2023-05-05T13:45:44.665362Z","iopub.execute_input":"2023-05-05T13:45:44.665728Z","iopub.status.idle":"2023-05-05T13:45:44.676968Z","shell.execute_reply.started":"2023-05-05T13:45:44.665698Z","shell.execute_reply":"2023-05-05T13:45:44.676077Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### Compute final set of hypernyms for given set of hyponyms","metadata":{}},{"cell_type":"code","source":"def compute_final_set_of_hypernyms(queries):\n    \n    final_set_of_hypernyms_given_query = {}\n\n    for query in queries:\n      # compute scores of each co-hyponym for given hyponyms\n      # score is calculated using the formula: score = cosine_similarity(co-hyponym,hyponym) * frequency(co-hyponym)\n      scores = {}\n      for co_hyponym in co_hyponyms_query[query]:\n        score = calculate_cosine_similarity_word2vec(query,co_hyponym)\n        scores[co_hyponym] = score * co_hyponyms_query[query][co_hyponym]\n\n      # append top(most similar) 15 co-hyponyms in Q\n      Q = []\n      Q.append(query)\n      scores = sorted(scores.items(), key=lambda x:x[1], reverse = True)\n      count = 0\n      for i in scores:\n#         if count == 15:\n#           break\n#         count += 1\n        Q.append(i[0])\n        \n      # Hq contains the list of hypernyms of the top 15 co-hyponyms\n      Hq = []\n      for q in Q:\n        Hq.extend(hyponym_hypernyms_eng[q])\n\n      # Compute frequency of hypernym as the count of hyponyms for which it is a hypernym \n      hypernym_freq = {}\n      for h in Hq:\n        c = 0\n        for cohyponym in co_hyponyms_query[query]:\n          if h in hyponym_hypernyms_eng[cohyponym]:\n            c += 1\n        hypernym_freq[h] = c\n\n      # Score each hypernym as follows: score = cosine_similarity(hypernym,original hyponym) * frequency(hypernym)^2\n      hypernym_scores = {}\n      Hq = set(Hq)\n      for h in Hq:\n        score = calculate_cosine_similarity_word2vec(query,h)\n        hypernym_scores[h] = score * hypernym_freq[h] * hypernym_freq[h]\n\n\n      # Take top 15 hypernyms as the final list of hypernyms for given set of hyponyms\n      final_set_hypernyms = []\n      hypernym_scores = sorted(hypernym_scores.items(), key=lambda x:x[1], reverse = True)\n      count = 0\n      for i in hypernym_scores:\n#         if count == 15:\n#           break\n#         count += 1\n        final_set_hypernyms.append(i[0])\n      \n      if len(final_set_hypernyms) > 100:\n        final_set_hypernyms = final_set_hypernyms[:100]\n        \n      final_set_of_hypernyms_given_query[query] = final_set_hypernyms\n      \n    return final_set_of_hypernyms_given_query","metadata":{"execution":{"iopub.status.busy":"2023-05-05T13:45:44.680064Z","iopub.execute_input":"2023-05-05T13:45:44.680342Z","iopub.status.idle":"2023-05-05T13:45:44.690415Z","shell.execute_reply.started":"2023-05-05T13:45:44.680319Z","shell.execute_reply":"2023-05-05T13:45:44.689519Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def predict_unsupervised(word):\n    if word not in data:\n        return []\n    else:\n        ans = compute_final_set_of_hypernyms([word])[word]\n        return ans","metadata":{"execution":{"iopub.status.busy":"2023-05-05T13:45:44.691767Z","iopub.execute_input":"2023-05-05T13:45:44.692271Z","iopub.status.idle":"2023-05-05T13:45:44.702955Z","shell.execute_reply.started":"2023-05-05T13:45:44.692239Z","shell.execute_reply":"2023-05-05T13:45:44.701969Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def top_15(hypernyms,input_query):\n    ans = []\n    scores = {}\n    for h in hypernyms:\n        sim = calculate_cosine_similarity_word2vec(h,input_query)\n        scores[h] = sim\n\n    scores = sorted(scores.items(), key=lambda x:x[1], reverse = True)\n    count = 0\n    for i in scores:\n        if input_query != i[0]:\n            if count == 15:\n              break\n            count += 1\n            ans.append(i[0])\n    return ans","metadata":{"execution":{"iopub.status.busy":"2023-05-05T13:45:44.705041Z","iopub.execute_input":"2023-05-05T13:45:44.705656Z","iopub.status.idle":"2023-05-05T13:45:44.713903Z","shell.execute_reply.started":"2023-05-05T13:45:44.705609Z","shell.execute_reply":"2023-05-05T13:45:44.712884Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def compute_hybrid_hypernyms(queries):\n    top_15_hypernyms = []\n    for q in tqdm(queries):\n        hypernyms_supervised = predict_supervised(q)\n        hypernyms_unsupervised = predict_unsupervised(q)\n        hypernyms = []\n        for h in hypernyms_supervised:\n            hypernyms.append(h)\n        hypernyms.extend(hypernyms_unsupervised)\n        hypernyms = set(hypernyms)\n        top15 = top_15(hypernyms,q)\n        top_15_hypernyms.append(top15)\n    return top_15_hypernyms","metadata":{"execution":{"iopub.status.busy":"2023-05-05T13:45:44.718048Z","iopub.execute_input":"2023-05-05T13:45:44.718301Z","iopub.status.idle":"2023-05-05T13:45:44.725639Z","shell.execute_reply.started":"2023-05-05T13:45:44.718281Z","shell.execute_reply":"2023-05-05T13:45:44.724633Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def write_to_file(final_list_hypernyms,filename):\n    f = open(filename, \"w\")\n    for i in final_list_hypernyms:\n        hypernyms = \"\"\n        for index,j in enumerate(i):\n            if index == len(i)-1:\n                hypernyms += str(j) + \"\\n\"\n            else:\n                hypernyms += str(j) + \"\\t\"\n        f.write(hypernyms)\n    f.close()","metadata":{"execution":{"iopub.status.busy":"2023-05-05T13:45:44.727866Z","iopub.execute_input":"2023-05-05T13:45:44.728621Z","iopub.status.idle":"2023-05-05T13:45:44.735899Z","shell.execute_reply.started":"2023-05-05T13:45:44.728590Z","shell.execute_reply":"2023-05-05T13:45:44.734861Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"queries = data\nfinal_hybrid_hypernyms = compute_hybrid_hypernyms(queries)\n#final_hybrid_hypernyms[0]","metadata":{"execution":{"iopub.status.busy":"2023-05-05T13:45:44.738800Z","iopub.execute_input":"2023-05-05T13:45:44.739710Z","iopub.status.idle":"2023-05-05T15:29:30.061414Z","shell.execute_reply.started":"2023-05-05T13:45:44.739679Z","shell.execute_reply":"2023-05-05T15:29:30.060487Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"100%|██████████| 1500/1500 [1:43:45<00:00,  4.15s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"# data[0]","metadata":{"execution":{"iopub.status.busy":"2023-05-05T15:29:30.063064Z","iopub.execute_input":"2023-05-05T15:29:30.063742Z","iopub.status.idle":"2023-05-05T15:29:30.067786Z","shell.execute_reply.started":"2023-05-05T15:29:30.063707Z","shell.execute_reply":"2023-05-05T15:29:30.066724Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def remove_underscores(final_hybrid_hypernyms):\n    for ind_sent,h_sent in enumerate(final_hybrid_hypernyms):\n        for ind_h,fh in enumerate(h_sent):\n            if '_' in fh:\n                h = fh.split('_')\n                hyp = \"\"\n                for i in range(len(h)):\n                    if i == len(h)-1:\n                        hyp += h[i]\n                    else:\n                        hyp += h[i] + \" \"\n                h_sent[ind_h] = hyp\n        final_hybrid_hypernyms[ind_sent] = h_sent\n    return final_hybrid_hypernyms","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:45:28.238638Z","iopub.execute_input":"2023-05-05T16:45:28.238996Z","iopub.status.idle":"2023-05-05T16:45:28.245714Z","shell.execute_reply.started":"2023-05-05T16:45:28.238967Z","shell.execute_reply":"2023-05-05T16:45:28.244171Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"final_hybrid_hypernyms = remove_underscores(final_hybrid_hypernyms)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:45:28.532670Z","iopub.execute_input":"2023-05-05T16:45:28.533333Z","iopub.status.idle":"2023-05-05T16:45:28.541258Z","shell.execute_reply.started":"2023-05-05T16:45:28.533292Z","shell.execute_reply":"2023-05-05T16:45:28.540363Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"write_to_file(final_hybrid_hypernyms,\"hypernyms.1A.english.training.txt\")","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:45:29.618933Z","iopub.execute_input":"2023-05-05T16:45:29.619595Z","iopub.status.idle":"2023-05-05T16:45:29.636663Z","shell.execute_reply.started":"2023-05-05T16:45:29.619560Z","shell.execute_reply":"2023-05-05T16:45:29.635794Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"## Compute top 15 hypernyms given query hyponym","metadata":{}},{"cell_type":"code","source":"q = 'pollution'","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:38:18.850010Z","iopub.execute_input":"2023-05-05T16:38:18.851087Z","iopub.status.idle":"2023-05-05T16:38:18.855548Z","shell.execute_reply.started":"2023-05-05T16:38:18.851040Z","shell.execute_reply":"2023-05-05T16:38:18.854395Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"print(\"Hyponym:\",q)\nprint(\"Top 15 hypernyms from supervised model\")\nhypernyms_supervised = predict_supervised(q)\nsup_top_15 = top_15(hypernyms_supervised,q)\nprint(sup_top_15)\nprint()\nprint(\"Top 15 hypernyms from unsupervised model\")\nhypernyms_unsupervised = predict_unsupervised(q)\nunsup_top_15 = top_15(hypernyms_unsupervised,q)\nprint(unsup_top_15)\nprint()\nprint(\"Top 15 hypernyms from hybrid model\")\nhypernyms = []\nfor h in hypernyms_supervised:\n    hypernyms.append(h)\nhypernyms.extend(hypernyms_unsupervised)\nhypernyms = set(hypernyms)\nhypernyms_final = top_15(hypernyms,q)\nprint(hypernyms_final)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-05T16:47:09.711741Z","iopub.execute_input":"2023-05-05T16:47:09.712085Z","iopub.status.idle":"2023-05-05T16:47:14.085620Z","shell.execute_reply.started":"2023-05-05T16:47:09.712058Z","shell.execute_reply":"2023-05-05T16:47:14.084694Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Hyponym: pollution\nTop 15 hypernyms from supervised model\n['person', 'chief', 'locale', 'boss', 'leader', 'politician', 'transmission', 'movement', 'channel', 'show', 'computer', 'competitor', 'worker', 'painter', 'wind']\n\nTop 15 hypernyms from unsupervised model\n['report', 'hearsay', 'impurity', 'junk', 'waste_material', 'defaecation', 'uncleanness', 'shit', 'excretion', 'find', 'granular_material', 'garbage', 'refuse', 'waste_product', 'dog_shit']\n\nTop 15 hypernyms from hybrid model\n['report', 'hearsay', 'person', 'impurity', 'junk', 'waste_material', 'defaecation', 'uncleanness', 'shit', 'excretion', 'find', 'granular_material', 'garbage', 'refuse', 'chief']\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}