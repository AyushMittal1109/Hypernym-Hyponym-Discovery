{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IMPORTS","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom collections import Counter\nimport random\nimport json\nimport re\nfrom sklearn.manifold import TSNE\nfrom scipy import spatial\nimport matplotlib.pyplot as plt\nimport pickle\nimport torch.nn.functional as F\nimport random\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(0)","metadata":{"execution":{"iopub.status.busy":"2023-05-03T19:59:27.357641Z","iopub.execute_input":"2023-05-03T19:59:27.358179Z","iopub.status.idle":"2023-05-03T19:59:27.370208Z","shell.execute_reply.started":"2023-05-03T19:59:27.358140Z","shell.execute_reply":"2023-05-03T19:59:27.368974Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7e5d62e4fc90>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Processing data and gold file","metadata":{}},{"cell_type":"code","source":"# preprocess data and gold file +  convert bi and trigrams to a underscore seperated word\n\nsubtask = \"2B.music\"\nphase = \"training\"\n\nf = open(f'/kaggle/input/inlp-project/{subtask}.{phase}.data.txt','r')\ndata = f.read()\nf.close()\nf = open(f'/kaggle/input/inlp-project/{subtask}.{phase}.gold.txt','r')\ngold = f.read()\nf.close()\nf = open(f'/kaggle/input/inlp-project/{subtask}.vocabulary.txt','r')\nVocab = f.read()\nf.close()\n\ndata = data.split('\\n')\ngold = gold.split('\\n')\nVocab = Vocab.split('\\n')","metadata":{"execution":{"iopub.status.busy":"2023-05-03T19:59:28.896775Z","iopub.execute_input":"2023-05-03T19:59:28.897519Z","iopub.status.idle":"2023-05-03T19:59:28.917527Z","shell.execute_reply.started":"2023-05-03T19:59:28.897479Z","shell.execute_reply":"2023-05-03T19:59:28.916453Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"w2i = {}\ni2w = {}\nvocab = []\nind = 1\n\nw2i['UNK'] = 0\ni2w[0] = 'UNK'\nvocab.append('UNK')\n\n\nfor line in tqdm(Vocab):\n    line = line.lower()\n    line = line.split(' ') \n    joined_word = \"\"\n    for w in line:\n        joined_word += w +\"_\"\n    joined_word = joined_word[:-1]\n    \n    w2i[joined_word] = ind\n    i2w[ind] = joined_word\n    vocab.append(joined_word)\n    ind += 1\n    \n        ","metadata":{"execution":{"iopub.status.busy":"2023-05-03T19:59:30.903878Z","iopub.execute_input":"2023-05-03T19:59:30.904826Z","iopub.status.idle":"2023-05-03T19:59:31.034251Z","shell.execute_reply.started":"2023-05-03T19:59:30.904774Z","shell.execute_reply":"2023-05-03T19:59:31.033091Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"100%|██████████| 69119/69119 [00:00<00:00, 606815.49it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"hyponyms = []\n\nfor line in data:\n    line = line.lower()\n    line = line.split(\"\\t\")\n    line = line[0]\n    line = line.split(\" \")\n    if len(line)>1:\n        joined_word = \"\"\n        for word in line:\n            joined_word += word + \"_\"\n        joined_word = joined_word[:-1]\n        if joined_word not in w2i.keys():\n            l = len(w2i.keys())\n            w2i[joined_word] = l\n            i2w[l] = joined_word\n            vocab.append(joined_word)\n        hyponyms.append(joined_word)\n    else:\n        hyponyms.append(line[0])\n        joined_word = line[0]\n        if joined_word not in w2i.keys():\n            l = len(w2i.keys())\n            w2i[joined_word] = l\n            i2w[l] = joined_word\n            vocab.append(joined_word)\nhyponyms = hyponyms[:-1]","metadata":{"execution":{"iopub.status.busy":"2023-05-03T19:59:35.551366Z","iopub.execute_input":"2023-05-03T19:59:35.553466Z","iopub.status.idle":"2023-05-03T19:59:35.565089Z","shell.execute_reply.started":"2023-05-03T19:59:35.553422Z","shell.execute_reply":"2023-05-03T19:59:35.563855Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"hyponyms[-10:]","metadata":{"execution":{"iopub.status.busy":"2023-05-03T20:03:45.459123Z","iopub.execute_input":"2023-05-03T20:03:45.460130Z","iopub.status.idle":"2023-05-03T20:03:45.467572Z","shell.execute_reply.started":"2023-05-03T20:03:45.460088Z","shell.execute_reply":"2023-05-03T20:03:45.466378Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"['melodic_phrase',\n 'hot_issue',\n 'gavotte',\n 'antiphon',\n 'recapitulation',\n 'fugazi',\n 'nightshift',\n 'solfeggio',\n 'dance_pop',\n 'zydeco']"},"metadata":{}}]},{"cell_type":"code","source":"hypernyms = []\nfor line in gold:\n    line = line.lower()\n    line = line.split(\"\\t\")\n    temp_hypernyms = []\n    for word in line:\n        word = word.split(\" \")\n        if len(word)>1:\n            joined_word = \"\"\n            for w in word:\n                joined_word += w + \"_\"\n            joined_word = joined_word[:-1]\n            if joined_word not in w2i.keys():\n                l = len(w2i.keys())\n                w2i[joined_word] = l\n                i2w[l] = joined_word\n                vocab.append(joined_word)\n            temp_hypernyms.append(joined_word)\n        else:\n            temp_hypernyms.append(word[0])\n            joined_word = word[0]\n            if joined_word not in w2i.keys():\n                l = len(w2i.keys())\n                w2i[joined_word] = l\n                i2w[l] = joined_word\n                vocab.append(joined_word)\n    hypernyms.append(temp_hypernyms)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-05-03T08:18:48.093619Z","iopub.execute_input":"2023-05-03T08:18:48.093994Z","iopub.status.idle":"2023-05-03T08:18:48.112790Z","shell.execute_reply.started":"2023-05-03T08:18:48.093959Z","shell.execute_reply":"2023-05-03T08:18:48.111571Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"all_hypernyms = set()\n\nfor line in hypernyms:\n    for word in line:\n        all_hypernyms.add(word)\n\nall_hypernyms = list(all_hypernyms)\n# all_hypernyms[:10]","metadata":{"execution":{"iopub.status.busy":"2023-05-03T08:18:49.565142Z","iopub.execute_input":"2023-05-03T08:18:49.565846Z","iopub.status.idle":"2023-05-03T08:18:49.572750Z","shell.execute_reply.started":"2023-05-03T08:18:49.565802Z","shell.execute_reply":"2023-05-03T08:18:49.570894Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# a function for finding negative hypernyms of given hyponyms\n# this function will return hyponym positive and negative hpyernyms in following manner\n# given hyponym - 'ayush'\n''' function should return - \n[\n\n    [['man'],['neg11','neg12','neg13','neg14','neg15']],\n    [['boy'],['neg21','neg22','neg23','neg24','neg25']],\n    [['person'],['neg31','neg32','neg33','neg34','neg35']],\n    [['student'],['neg41','neg42','neg43','neg44','neg45']],\n    \n    ]\n    \n    \n    '''\n\nnum_neg_hypernyms = 5\n\ndef pos_neg_hypernyms(hyponym):\n\n    try:\n        index_in_data = hyponyms.index(hyponym)\n        \n    except:\n        print(ind,len(hyponyms),hyponym)\n    \n    hypernyms_temp = hypernyms[index_in_data]\n    num_hypernyms = len(hypernyms_temp)\n    neg_hypernyms = []\n    for i in range(num_hypernyms*num_neg_hypernyms):\n        neg_h = all_hypernyms[random.randint(0,len(all_hypernyms)-1)]\n        while neg_h in neg_hypernyms or neg_h == hyponym or neg_h in hypernyms_temp: \n            neg_h = all_hypernyms[random.randint(0,len(all_hypernyms)-1)]\n            \n        neg_hypernyms.append(neg_h)\n    \n    ans = []\n    for i in range(num_hypernyms):\n        ans_temp = []\n        h_ind = w2i[hypernyms_temp[i]]\n        ans_temp.append([h_ind])\n        \n                    \n        neg_temp = []\n        for j in range(i*5,i*5+5):\n            neg_temp.append(w2i[neg_hypernyms[j]])\n            \n        ans_temp.append(neg_temp)\n        \n        ans.append(ans_temp)\n    return ans\n        ","metadata":{"execution":{"iopub.status.busy":"2023-05-03T08:18:51.423214Z","iopub.execute_input":"2023-05-03T08:18:51.423578Z","iopub.status.idle":"2023-05-03T08:18:51.434552Z","shell.execute_reply.started":"2023-05-03T08:18:51.423544Z","shell.execute_reply":"2023-05-03T08:18:51.433236Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"# Initilization of embedding from word2vec","metadata":{}},{"cell_type":"code","source":"# embed_dict = {}\n\n# with open('/kaggle/input/glove-embeddings/glove.6B.300d.txt','r') as f:\n#     for line in f:\n#         values = line.split()\n#         word = values[0]\n#         vector = np.asarray(values[1:],'float32')\n#         embed_dict[word]=vector\n\n# embed_dict['oov'] = np.zeros(300)\n\n\nf = open('/kaggle/input/word2vec/model.txt','r')\nword2vec_pretrained = f.read()\nword2vec_pretrained = word2vec_pretrained.split('\\n')\nword_emb = {}\nfor i,sent in tqdm(enumerate(word2vec_pretrained)):\n    if i == 0 or i == len(word2vec_pretrained)-1:\n        continue\n    sent = sent.split(' ')\n    word_tag = sent[0]\n    word_tag = word_tag.split('_')\n    word = word_tag[0]\n    tag = word_tag[1]\n    emb = sent[1:]\n    word_emb[word] = emb","metadata":{"execution":{"iopub.status.busy":"2023-05-03T08:18:53.897979Z","iopub.execute_input":"2023-05-03T08:18:53.899148Z","iopub.status.idle":"2023-05-03T08:19:05.752771Z","shell.execute_reply.started":"2023-05-03T08:18:53.899101Z","shell.execute_reply":"2023-05-03T08:19:05.751708Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"163475it [00:06, 23552.59it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"my_embed = torch.empty((len(w2i.keys()),300),dtype=torch.float32).to(device)\n\nfor i in tqdm(range(len(w2i.keys()))):\n    try:\n        my_embed[i] = tensor.torch(word_emb[i2w[i]])\n    except:\n        my_embed[i] = torch.randn(300) - 0.5\n#     my_embed.append(x)\n    \n# my_embed = np.array(my_embed)","metadata":{"execution":{"iopub.status.busy":"2023-05-03T08:19:05.755165Z","iopub.execute_input":"2023-05-03T08:19:05.755898Z","iopub.status.idle":"2023-05-03T08:19:07.990988Z","shell.execute_reply.started":"2023-05-03T08:19:05.755858Z","shell.execute_reply":"2023-05-03T08:19:07.989822Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"100%|██████████| 69214/69214 [00:02<00:00, 31704.77it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(type(my_embed[0]))\nayush = torch.tensor(my_embed).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-05-03T08:19:07.992699Z","iopub.execute_input":"2023-05-03T08:19:07.993121Z","iopub.status.idle":"2023-05-03T08:19:08.000485Z","shell.execute_reply.started":"2023-05-03T08:19:07.993082Z","shell.execute_reply":"2023-05-03T08:19:07.999325Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"<class 'torch.Tensor'>\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  \n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model architecture","metadata":{}},{"cell_type":"code","source":"class w2v_HH_embeddings(nn.Module):\n    def __init__(self, vocab_size, embedding_size):\n        super(w2v_HH_embeddings, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_size)\n        self.embedding.weight.data.copy_(my_embed) #to do\n        \n        self.linear1 = nn.Linear(embedding_size, 1)\n        self.linear2 = nn.Linear(embedding_size, 1)\n        # self.sigmoid = nn.Sigmoid()\n\n    def forward(self, hyponym, hypernym, neg_hypernym):\n        # (bs,1) (bs,1) (bs,neg_sam)\n        hyponym_embeddings = self.embedding(hyponym) # bs,1,300\n        hypernym_embeddings = self.embedding(hypernym) # bs,1,300\n        neg_hypernym_embeddings = self.embedding(neg_hypernym) # bs,5,300\n        \n#         similarity between hyponym and true hypernym\n        pos_score = torch.mul(hyponym_embeddings, hypernym_embeddings) #bs,1,300\n        pos_score = torch.squeeze(pos_score, 1)#bs,300  \n        pos_score = self.linear1(pos_score)#bs,1        \n        pos_score = -F.logsigmoid(pos_score) #bs,1\n        pos_score = torch.squeeze(pos_score,1) #bs\n\n#         similarity between hyponym and true neg hypernym\n        hyponym_embeddingsT = torch.transpose(hyponym_embeddings, 1, 2) #bs,300, 1\n        neg_score = torch.bmm(neg_hypernym_embeddings, hyponym_embeddingsT) #bs,5,1\n        neg_score = torch.squeeze(neg_score, 2)#bs,5\n        neg_score = -F.logsigmoid(-neg_score) #bs,5\n        neg_score = torch.sum(neg_score,dim=1) # bs\n        total_score = torch.mean(pos_score + neg_score)\n        return total_score","metadata":{"execution":{"iopub.status.busy":"2023-05-03T08:19:08.021791Z","iopub.execute_input":"2023-05-03T08:19:08.022705Z","iopub.status.idle":"2023-05-03T08:19:08.033594Z","shell.execute_reply.started":"2023-05-03T08:19:08.022669Z","shell.execute_reply":"2023-05-03T08:19:08.032848Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"# Parameters","metadata":{}},{"cell_type":"code","source":"vocab_size = len(i2w.keys())\nepochs = 50\nbatch_size = 32","metadata":{"execution":{"iopub.status.busy":"2023-05-03T08:19:08.034799Z","iopub.execute_input":"2023-05-03T08:19:08.035635Z","iopub.status.idle":"2023-05-03T08:19:08.048663Z","shell.execute_reply.started":"2023-05-03T08:19:08.035590Z","shell.execute_reply":"2023-05-03T08:19:08.047549Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"model = w2v_HH_embeddings(vocab_size,300)\nmodel.to(device)\noptimizer = optim.Adam(model.parameters(),lr=0.0001)","metadata":{"execution":{"iopub.status.busy":"2023-05-03T08:19:08.051454Z","iopub.execute_input":"2023-05-03T08:19:08.051746Z","iopub.status.idle":"2023-05-03T08:19:08.267214Z","shell.execute_reply.started":"2023-05-03T08:19:08.051707Z","shell.execute_reply":"2023-05-03T08:19:08.266035Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"# Training the embeddings","metadata":{}},{"cell_type":"code","source":"for epoch in range(epochs):\n    hypernym_batch = []\n    hyponym_batch = []\n    neg_hypernym_batch = []\n    running_loss = []\n    \n    for i,hyponym in tqdm(enumerate(hyponyms)):\n#         ind = w2i[hyponyms[i]]\n        temp = pos_neg_hypernyms(hyponym)\n        \n        for a_list in temp:\n            # (bs,1) (bs,1) (bs,neg_sam)\n            hyponym_batch.append([ind]) #bs*1\n            \n            hypernym_batch.append(a_list[0]) # bs*1\n            neg_hypernym_batch.append(a_list[1]) # bs*5\n            \n            if len(hyponym_batch) == batch_size:\n                a = torch.tensor(hyponym_batch).to(device) # bs*1\n                \n                b = torch.tensor(hypernym_batch).to(device) # bs*1\n                \n                c = torch.tensor(neg_hypernym_batch).to(device) # bs*5\n                \n                loss = model(a,b,c)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                running_loss.append(loss.item())\n                \n                hyponym_batch.clear()\n                hypernym_batch.clear()\n                neg_hypernym_batch.clear()\n                \n    epoch_loss = np.mean(running_loss)\n    print(\"training epoch_loss is\", epoch_loss)\n                \n#         a = torch.tensor(hyponym_batch)\n#         b = torch.tensor(hypernym_batch)\n#         c = torch.tensor(neg_hypernym_batch)\n#         similarity = model(a,b,c)\n''' function should return - \n[\n\n    [['man'],['neg11','neg12','neg13','neg14','neg15']],\n    [['boy'],['neg21','neg22','neg23','neg24','neg25']],\n    [['person'],['neg31','neg32','neg33','neg34','neg35']],\n    [['student'],['neg41','neg42','neg43','neg44','neg45']],\n    \n    ]\n    \n    \n    '''","metadata":{"execution":{"iopub.status.busy":"2023-05-03T08:19:08.283998Z","iopub.execute_input":"2023-05-03T08:19:08.285310Z","iopub.status.idle":"2023-05-03T08:20:06.196897Z","shell.execute_reply.started":"2023-05-03T08:19:08.285269Z","shell.execute_reply":"2023-05-03T08:20:06.195823Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"499it [00:01, 431.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 336.03765061322383\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 436.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 313.1902713551241\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 432.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 291.8517249612247\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 438.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 269.1547970042509\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 442.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 248.03088387882008\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 437.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 227.13322358972886\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 434.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 204.3816203397863\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 435.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 182.01185204001035\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 424.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 162.69990369011373\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 433.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 141.3122401966768\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 432.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 121.30799663768096\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 435.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 101.20090103149414\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 438.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 84.5836877710679\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 437.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 69.41216924330767\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 431.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 55.03534128525678\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 443.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 43.96806541891659\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 443.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 34.33574177237118\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 438.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 26.541035079956053\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 423.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 20.085487898658304\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 414.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 14.573886910606833\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 399.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 10.814496332056382\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 440.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 7.898683971517226\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 439.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 5.584952235221863\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 439.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 4.256281128350426\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 438.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 3.0173813285196527\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 436.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 2.1856285459855025\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 437.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 1.5370405879090814\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 423.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 1.1260290913283826\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 432.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 0.8013097006608458\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 438.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 0.6005954576108385\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 438.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 0.505729165213073\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 439.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 0.4913382186280454\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 440.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 0.3614407949587878\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 438.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 0.34079958696247026\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 439.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 0.2754429548047483\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 441.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 0.2503763465046444\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 437.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 0.2048667786478558\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 422.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 0.2052698367521824\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 440.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 0.14010505785015137\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 438.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 0.15102387710255297\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 438.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 0.10606016990838243\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 440.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 0.09949690085452269\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 441.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 0.06850692567360751\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 439.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 0.04045850287246353\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 439.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 0.021113137025660013\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 434.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 0.010848208074457943\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 422.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 0.007108109299203053\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 360.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 0.005947790813961011\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 440.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 0.005055907646687154\n","output_type":"stream"},{"name":"stderr","text":"499it [00:01, 438.87it/s]","output_type":"stream"},{"name":"stdout","text":"training epoch_loss is 0.004283288653994746\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"\" function should return - \\n[\\n\\n    [['man'],['neg11','neg12','neg13','neg14','neg15']],\\n    [['boy'],['neg21','neg22','neg23','neg24','neg25']],\\n    [['person'],['neg31','neg32','neg33','neg34','neg35']],\\n    [['student'],['neg41','neg42','neg43','neg44','neg45']],\\n    \\n    ]\\n    \\n    \\n    \""},"metadata":{}}]},{"cell_type":"code","source":"torch.save(model, '/kaggle/working/hypernym-hyponym-embeddings_training.pt')","metadata":{"execution":{"iopub.status.busy":"2023-05-03T08:20:06.200174Z","iopub.execute_input":"2023-05-03T08:20:06.200483Z","iopub.status.idle":"2023-05-03T08:20:06.369438Z","shell.execute_reply.started":"2023-05-03T08:20:06.200455Z","shell.execute_reply":"2023-05-03T08:20:06.368321Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"saved_embeddings = {}\nfor i in tqdm(range(1,len(i2w.keys()))):#(len(index2word)):\n    word = i2w[i]\n    saved_embeddings[word] = model.embedding.weight[i].detach().cpu().numpy()\n\nwith open('hypernym-hyponym-embeddings_2B.pkl','wb') as f:\n    pickle.dump(saved_embeddings,f)","metadata":{"execution":{"iopub.status.busy":"2023-05-03T08:20:16.739406Z","iopub.execute_input":"2023-05-03T08:20:16.739989Z","iopub.status.idle":"2023-05-03T08:20:20.449989Z","shell.execute_reply.started":"2023-05-03T08:20:16.739952Z","shell.execute_reply":"2023-05-03T08:20:20.448772Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stderr","text":"100%|██████████| 69213/69213 [00:02<00:00, 23812.00it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"parameters = {}\n\nparameters['vocab'] = vocab\nparameters['i2w'] = i2w\nparameters['w2i'] = w2i\nparameters['hypernyms'] = hypernyms\nparameters['hyponyms'] = hyponyms\n\nwith open('hypernym-hyponym-dictionaries_2B.pkl','wb') as f:\n    pickle.dump(parameters,f)","metadata":{"execution":{"iopub.status.busy":"2023-05-03T08:20:25.146090Z","iopub.execute_input":"2023-05-03T08:20:25.146788Z","iopub.status.idle":"2023-05-03T08:20:25.189404Z","shell.execute_reply.started":"2023-05-03T08:20:25.146753Z","shell.execute_reply":"2023-05-03T08:20:25.188359Z"},"trusted":true},"execution_count":46,"outputs":[]}]}