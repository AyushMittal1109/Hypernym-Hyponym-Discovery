{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# IMPORTS","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom collections import Counter\nimport random\nimport json\nimport re\nfrom sklearn.manifold import TSNE\nfrom scipy import spatial\nimport matplotlib.pyplot as plt\nimport pickle\nimport copy\nfrom tqdm import tqdm\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(0)","metadata":{"execution":{"iopub.status.busy":"2023-05-03T10:48:59.592532Z","iopub.execute_input":"2023-05-03T10:48:59.593022Z","iopub.status.idle":"2023-05-03T10:48:59.606678Z","shell.execute_reply.started":"2023-05-03T10:48:59.592977Z","shell.execute_reply":"2023-05-03T10:48:59.605639Z"},"trusted":true},"execution_count":63,"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7d687fb2c150>"},"metadata":{}}]},{"cell_type":"markdown","source":"\n# Parameters","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data loading","metadata":{}},{"cell_type":"code","source":"file = open(\"/kaggle/input/inlp-project/hypernym-hyponym-dictionaries_2B.pkl\",'rb')\nparameters = pickle.load(file)\nfile.close()\n\nvocab = parameters['vocab']\nword2index = parameters['w2i']\nindex2word = parameters['i2w']\n","metadata":{"execution":{"iopub.status.busy":"2023-05-03T10:49:01.647581Z","iopub.execute_input":"2023-05-03T10:49:01.648328Z","iopub.status.idle":"2023-05-03T10:49:01.684522Z","shell.execute_reply.started":"2023-05-03T10:49:01.648291Z","shell.execute_reply":"2023-05-03T10:49:01.683523Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"print(word2index[\"bagpipe\"])","metadata":{"execution":{"iopub.status.busy":"2023-05-03T10:49:04.388198Z","iopub.execute_input":"2023-05-03T10:49:04.389220Z","iopub.status.idle":"2023-05-03T10:49:04.395865Z","shell.execute_reply.started":"2023-05-03T10:49:04.389170Z","shell.execute_reply":"2023-05-03T10:49:04.394703Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"4257\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Parameters","metadata":{}},{"cell_type":"code","source":"# no of projection matrices\nk = 24\n\n# no of dimentions in embedding\ndim = 300\n\n# no of negative samples\nneg_sample_count = 5\n\n# learning rate\nlr = 0.001\n\nbatch_size = 32\n\nvocab_size = len(vocab)\n\nphase = \"training\"\nsubtask = \"2B.music\"\n\n# datafile\ndataFilePath = f\"/kaggle/input/inlp-project/{subtask}.{phase}.data.txt\"\n\n# goldfile\ngoldFilePath = f\"/kaggle/input/inlp-project/{subtask}.{phase}.gold.txt\"\n\n# vocab\nvocabFilePath = f\"/kaggle/input/inlp-project/{subtask}.vocabulary.txt\"\n","metadata":{"execution":{"iopub.status.busy":"2023-05-03T10:49:12.473569Z","iopub.execute_input":"2023-05-03T10:49:12.474360Z","iopub.status.idle":"2023-05-03T10:49:12.482778Z","shell.execute_reply.started":"2023-05-03T10:49:12.474321Z","shell.execute_reply":"2023-05-03T10:49:12.481658Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"# vocab = []\n# with open(vocabFilePath) as dataset:\n#     for line in tqdm(dataset):\n# #         print(line)\n#         line = line.split('\\t')\n#         vocab.append(line[0][:-1])\n# vocab_size = len(vocab)\n# print(vocab[:20],vocab_size)","metadata":{"execution":{"iopub.status.busy":"2023-05-03T10:49:13.153019Z","iopub.execute_input":"2023-05-03T10:49:13.153705Z","iopub.status.idle":"2023-05-03T10:49:13.158245Z","shell.execute_reply.started":"2023-05-03T10:49:13.153668Z","shell.execute_reply":"2023-05-03T10:49:13.157002Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"# word2index = {}\n# index2word = {}\n\n# word2index['UNK'] = 0\n# index2word[0] = 'UNK'\n# for i,word in enumerate(vocab):\n#     word2index[word] = i+1\n#     index2word[i] = word\n    \n# print(list(word2index.keys())[:20])","metadata":{"execution":{"iopub.status.busy":"2023-05-03T10:49:13.711428Z","iopub.execute_input":"2023-05-03T10:49:13.712536Z","iopub.status.idle":"2023-05-03T10:49:13.717397Z","shell.execute_reply.started":"2023-05-03T10:49:13.712487Z","shell.execute_reply":"2023-05-03T10:49:13.716310Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"# considering preprocesses data like lower and three gram, bi gram, one gram\n\ndata = []\nwith open(dataFilePath) as dataset:\n    for line in tqdm(dataset):\n        line = line.lower()\n        line = line.split('\\t')\n        data.append(line[0])\n        \nprint(len(data))","metadata":{"execution":{"iopub.status.busy":"2023-05-03T10:49:15.674303Z","iopub.execute_input":"2023-05-03T10:49:15.674716Z","iopub.status.idle":"2023-05-03T10:49:15.687257Z","shell.execute_reply.started":"2023-05-03T10:49:15.674681Z","shell.execute_reply":"2023-05-03T10:49:15.686151Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stderr","text":"500it [00:00, 472437.94it/s]","output_type":"stream"},{"name":"stdout","text":"500\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"print(data[:20])","metadata":{"execution":{"iopub.status.busy":"2023-05-03T10:49:26.461187Z","iopub.execute_input":"2023-05-03T10:49:26.462014Z","iopub.status.idle":"2023-05-03T10:49:26.467522Z","shell.execute_reply.started":"2023-05-03T10:49:26.461973Z","shell.execute_reply":"2023-05-03T10:49:26.466410Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"['bagpipe', 'group therapy', 'coda', 'dave brockie', 'third stage', 'preface', 'heartland rock', 'dance-rock', 'gittler guitar', 'latin hip-hop', 'passage', 'nirvana', 'keyboardist', 'zeraphine', 'massive attack', 'anticlimax', 'mikey chung', 'grapevyne', 'suspended cymbal', 'trap house']\n","output_type":"stream"}]},{"cell_type":"code","source":"gold = []\nwith open(goldFilePath) as dataset:\n    for line in tqdm(dataset):\n        line = line.lower()\n        line = line.strip()\n        line = line.split('\\t')\n        gold.append(line)\n        \nprint(gold[:20])","metadata":{"execution":{"iopub.status.busy":"2023-05-03T10:49:29.935164Z","iopub.execute_input":"2023-05-03T10:49:29.936101Z","iopub.status.idle":"2023-05-03T10:49:29.950985Z","shell.execute_reply.started":"2023-05-03T10:49:29.936049Z","shell.execute_reply":"2023-05-03T10:49:29.949709Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stderr","text":"500it [00:00, 191713.32it/s]","output_type":"stream"},{"name":"stdout","text":"[['instrument', 'musical instrument', 'pipe', 'wind', 'wind instrument', 'aerophone'], ['album', 'record album', 'studio album', 'musical work', 'audio signal', 'medium', 'work of art', 'signal', 'electrical energy', 'function', 'storage'], ['movement', 'end', 'ending', 'close', 'closing', 'cul-de-sac', 'conclusion', 'musical work', 'musical composition', 'piece of music', 'opus', 'composition', 'freeway', 'thoroughfare', 'pike', 'turnpike', 'expressway', 'work of art', 'route', 'way', 'infrastructure', 'base', 'fund', 'store', 'stock'], ['lead vocalist', 'person'], ['album', 'record album', 'studio album', 'musical work', 'audio signal', 'medium', 'work of art', 'signal', 'electrical energy', 'function', 'storage'], ['introduction'], ['musical style', 'music genre'], ['musical style', 'music genre'], ['guitar', 'stringed instrument', 'string instrument', 'plucked instrument', 'plucked string instrument', 'instrument', 'musical instrument'], ['latin music', 'music genre'], ['musical composition', 'piece of music', 'opus', 'composition', 'musical work', 'work of art'], ['album', 'record album', 'studio album', 'extended play', 'vocal', 'song', 'track', 'human voice', 'split album', 'single', 'band', 'musical group', 'musical organization', 'music group', 'bandmember', 'rock group', 'rock band', 'team', 'medium', 'compilation album', 'musical work', 'audio signal', 'audio recording', 'sound recording', 'audio', 'sound reproduction', 'sound system', 'audio system', 'musical composition', 'piece of music', 'opus', 'composition', 'release', 'dance band', 'dance orchestra', 'work of art', 'signal', 'recording', 'reproduction', 'disc jockey', 'disk jockey', 'dj', 'electronic equipment', 'storage', 'electrical energy', 'function', 'task', 'repeating', 'repetition'], ['musician', 'person'], ['band', 'musical group', 'musical organization', 'music group', 'bandmember', 'rock group', 'rock band', 'team', 'alternative rock', 'dance band', 'dance orchestra', 'rock and roll', 'rock music', 'rock-and-roll', 'rock', \"rock'n'roll\", 'musical style', 'music genre', 'popular music', 'popular music genre'], ['single', 'medium', 'vocal', 'song', 'track', 'human voice', 'release', 'musical composition', 'piece of music', 'opus', 'composition', 'musical work', 'storage', 'work of art'], ['close', 'closing', 'conclusion', 'end', 'ending'], ['record producer', 'person', 'boss', 'chief', 'leader', 'administration', 'direction', 'management', 'corporate executive', 'business executive', 'executive', 'executive director', 'title', 'firm', 'corporation', 'company', 'partnership', 'venture', 'enterprise', 'concern', 'administrator', 'honorific'], ['medium', 'single', 'release', 'vocal', 'song', 'track', 'human voice', 'musical composition', 'piece of music', 'opus', 'composition', 'storage', 'musical work', 'work of art'], ['cymbal', 'percussion', 'percussive instrument', 'percussion instrument', 'idiophone', 'instrument', 'musical instrument'], ['independent music', 'album', 'record album', 'studio album', 'diy ethic', 'musical work', 'audio signal', 'medium', 'work of art', 'signal', 'electrical energy', 'function', 'storage']]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"def compute_hyponym_hypernyms(data_train_sent,gold_train_sent):\n    hyponym_hypernyms = {}\n    for i in range(len(data_train_sent)):\n        hyponym = data_train_sent[i]\n        hypernyms = gold_train_sent[i]\n        hyponym_hypernyms[hyponym] = hypernyms\n    return hyponym_hypernyms","metadata":{"execution":{"iopub.status.busy":"2023-05-03T10:49:51.643077Z","iopub.execute_input":"2023-05-03T10:49:51.643689Z","iopub.status.idle":"2023-05-03T10:49:51.649150Z","shell.execute_reply.started":"2023-05-03T10:49:51.643650Z","shell.execute_reply":"2023-05-03T10:49:51.648015Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"hyponym_hypernyms = compute_hyponym_hypernyms(data,gold)","metadata":{"execution":{"iopub.status.busy":"2023-05-03T10:49:53.027087Z","iopub.execute_input":"2023-05-03T10:49:53.027460Z","iopub.status.idle":"2023-05-03T10:49:53.032856Z","shell.execute_reply.started":"2023-05-03T10:49:53.027426Z","shell.execute_reply":"2023-05-03T10:49:53.031624Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"hyponym_hypernyms['bagpipe']","metadata":{"execution":{"iopub.status.busy":"2023-05-03T10:50:16.914097Z","iopub.execute_input":"2023-05-03T10:50:16.914704Z","iopub.status.idle":"2023-05-03T10:50:16.922041Z","shell.execute_reply.started":"2023-05-03T10:50:16.914664Z","shell.execute_reply":"2023-05-03T10:50:16.920849Z"},"trusted":true},"execution_count":75,"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"['instrument',\n 'musical instrument',\n 'pipe',\n 'wind',\n 'wind instrument',\n 'aerophone']"},"metadata":{}}]},{"cell_type":"markdown","source":"# MODEL ARCHITECTURE","metadata":{}},{"cell_type":"markdown","source":"# loading embeddings","metadata":{}},{"cell_type":"code","source":"file = open(\"/kaggle/input/inlp-project/hypernym-hyponym-embeddings_2B.pkl\",'rb')\nembedding = pickle.load(file)\nfile.close()\n\ntrained_embeddings = torch.randn(dim).to(device)\n\nfor word in tqdm(vocab[1:]):\n    x = torch.from_numpy(embedding[word]).to(device)\n    x = x.reshape([-1])\n    trained_embeddings = torch.cat((trained_embeddings,x))\ntrained_embeddings = trained_embeddings.reshape([-1,dim])\n# trained_embeddings = torch.empty((vocab_size, dim),dtype=torch.float32).to(device)\n# for i,word in tqdm(enumerate(vocab)):\n#     trained_embeddings[i] = torch.from_numpy(embedding[word]).to(device)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-03T10:50:21.256662Z","iopub.execute_input":"2023-05-03T10:50:21.257037Z","iopub.status.idle":"2023-05-03T10:50:35.175773Z","shell.execute_reply.started":"2023-05-03T10:50:21.257004Z","shell.execute_reply":"2023-05-03T10:50:35.174652Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stderr","text":"100%|██████████| 69213/69213 [00:13<00:00, 5039.94it/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"class HHD(nn.Module):\n    def __init__(self, vocab_size, embedding_size):\n        super(HHD, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_size)\n        self.embedding.weight.data.copy_(trained_embeddings) #to do\n\n        self.output = nn.Linear(k, 1)\n        \n        var = 2 / (dim + dim)\n        \n        # Initialize projection matrices using scheme from Glorot & Bengio (2008).\n        \n        self.proj_mats = torch.zeros([k, dim, dim], dtype=torch.float32).to(device)\n        # Fills self tensor with elements samples from the normal distribution parameterized by mean and std.\n        self.proj_mats.normal_(0, var)\n        # mat_data is of size k*dim*dim\n        # finally mat_data is k*dim*dim matrix ie k projection matrices, each matric is populated with random value\n        # diagonal elements will be 1+random value and other will be 0+random value and random value will range 0 and var\n        self.proj_mats += torch.cat([torch.eye(dim, ).unsqueeze(0) for _ in range(k)]).to(device)\n        self.sigmoid = nn.Sigmoid()\n        \n#     def similarity(self,query, cand_hypernym):\n#         query = self.embedding(query) #1*d\n#         cand_hypernymT = self.embedding(cand_hypernym) #1*d\n        \n#         #proj is of dim d*d, q is 1*d\n#         qT = torch.transpose(query,0,1).to(device) # d*1\n#         projT = torch.matmul(self.proj_mats,qT).to(device) #k*d*d X d*1 = k*d*1\n#         projT = torch.squeeze(projT,2).to(device) #k*d\n#         proj = torch.transpose(projT,0,1).to(device) #d*k\n        \n#         # find similarity between query and candidate \n#         cand_hypernym = torch.transpose(cand_hypernymT,0,1) #d*1\n#         simPosHyper = torch.matmul(projT,cand_hypernym).to(device) #k*d x d*1 = k*1\n#         simPosHyper = torch.squeeze(simPosHyper,1) # k\n#         simPos = self.output(simPosHyper) # 1\n        \n#         return simPos\n    \n    def similarity(self,query, cand_hypernym,bs):\n        query = self.embedding(query) #1*d\n        cand_hypernymT = self.embedding(cand_hypernym) #bs*d\n        \n        #proj is of dim d*d, q is 1*d\n        qT = torch.transpose(query,0,1).to(device) # d*1\n        projT = torch.matmul(self.proj_mats,qT).to(device) #k*d*d X d*1 = k*d*1\n        projT = torch.squeeze(projT,2).to(device) #k*d\n        proj = torch.transpose(projT,0,1).to(device) #d*k\n        \n        # find similarity between query and candidate \n        cand_hypernym = torch.transpose(cand_hypernymT,0,1) #d*bs\n        simPosHyper = torch.matmul(projT,cand_hypernym).to(device) #k*d x d*bs = k*bs\n#         simPosHyper = torch.squeeze(simPosHyper,1) # k\n        simPosHyper = torch.transpose(simPosHyper,0,1) # bs*k\n        simPos = self.output(simPosHyper) # bs*1\n        simPos = self.sigmoid(simPos) #bs*1\n        \n        return simPos\n        \n\n    def forward(self, query, cand_hypernym, neg_hypernyms ):\n        # query - 255 , cand_hypernym - 255, neg_hypernyms - 255*5\n        # getting embeddings of required entities\n        query = self.embedding(query) #bs*d\n        cand_hypernymT = self.embedding(cand_hypernym) #bs*d\n        neg_hypernymsT = self.embedding(neg_hypernyms) #bs*ns*d\n        \n        query = torch.unsqueeze(query,2) # bs*d*1\n\n#         batch_proj = torch.empty((batch_size,k,dim),dtype=torch.float32).to(device)\n#         for i,q in enumerate(query):\n#             # q is tensor of size d*1\n#             projT = torch.matmul(self.proj_mats,q).to(device) # k*d*d X d*1 = k*d*1\n#             projT = torch.squeeze(projT,2) # k*d\n#             batch_proj[i] = projT #bs*k*d\n            \n        batch_proj = torch.tensor([]).to(device)\n        for i,q in enumerate(query):\n            projT = torch.matmul(self.proj_mats,q).to(device) # k*d*d X d*1 = k*d*1\n            projT = torch.squeeze(projT,2) # k*d\n            projT = projT.reshape([-1])\n            batch_proj = torch.cat((batch_proj,projT))\n        \n        batch_proj = batch_proj.reshape([-1,k,dim])\n        \n        \n        # find similarity between query and candidate \n        cand_hypernym = torch.unsqueeze(cand_hypernymT,2) #bs*d*1\n        simPos = torch.bmm(batch_proj,cand_hypernym) #bs*k*d x bs*d*1 = bs*k*1\n        simPos = torch.squeeze(simPos,2) #bs*k\n        simPosOutput = self.output(simPos) #bs*1\n#         simPosOutput = self.sigmoid(simPosOutput)\n        \n        \n        # a step from above\n        # find similarity between query and negative samples\n        batch_projT = torch.transpose(batch_proj,1,2) #bs*d*k\n        simNegs = torch.bmm(neg_hypernymsT,batch_projT) #bs*ns*d x bs*d*k = bs*ns*k\n        simNegsOutput = self.output(simNegs) #bs*ns*1\n        simNegsOutput = torch.squeeze(simNegsOutput,2) #bs*ns\n#         simNegsOutput = self.sigmoid(simNegsOutput)\n        \n        \n        \n        # simPos - bs*1, simNegs - bs*ns\n        return simPosOutput,simNegsOutput\n    \n#     ///////////////////////////////////////////////////////////////////////////////////////\n        # getting embeddings of required entities\n        query = self.embedding(query)\n        cand_hypernymT = self.embedding(cand_hypernym) #1*d\n        neg_hypernymsT = self.embedding(neg_hypernyms) #ns*1*d\n        \n        #proj is of dim d*d, q is 1*d\n        qT = torch.transpose(query,0,1) # d*1\n        projT = torch.matmul(self.proj_mats,qT).to(device)\n        projT = torch.squeeze(projT,2) # k*d*d X d*1 = k*d*1\n        proj = torch.transpose(projT,0,1) #k*d\n        \n        # find similarity between query and candidate \n        cand_hypernym = torch.transpose(cand_hypernymT,0,1) #d*1\n        simPosHyper = torch.matmul(proj,cand_hypernym).to(device) #k*d x d*1 = k*1\n        simPosHyper = torch.squeeze(simPosHyper,1) # k\n        simPos = output(simPosHyper) # 1\n        \n        # find similarity between query and negative samples\n        #neg_hypernyms = torch.transpose(neg_hypernymsT,1,2) # ns*d*1\n        simNegHypersT = torch.matmul(neg_hypernymsT,projT).to(device) # ns*1*d x d*k= ns*1*k\n        simNegHypers = torch.transpose(simNegHypersT,1,2) # ns*k*1\n        simNegHypers = torch.squeeze(simNegHypers,2) # ns*k\n        simNegs = output(simNegHypers) # ns*1\n        simNegs = torch.squeeze(simNegs,1) # ns\n        \n        return simPos,simNegs\n    \n                \n        \n        \n","metadata":{"execution":{"iopub.status.busy":"2023-05-03T10:50:38.105498Z","iopub.execute_input":"2023-05-03T10:50:38.106270Z","iopub.status.idle":"2023-05-03T10:50:38.127839Z","shell.execute_reply.started":"2023-05-03T10:50:38.106216Z","shell.execute_reply":"2023-05-03T10:50:38.126642Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"if \"bagpipe\" in hyponyms:\n    print(\"Ayush\")","metadata":{"execution":{"iopub.status.busy":"2023-05-03T10:48:27.889330Z","iopub.execute_input":"2023-05-03T10:48:27.889813Z","iopub.status.idle":"2023-05-03T10:48:27.910208Z","shell.execute_reply.started":"2023-05-03T10:48:27.889775Z","shell.execute_reply":"2023-05-03T10:48:27.908826Z"},"trusted":true},"execution_count":62,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/1303097479.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;34m\"bagpipe\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhyponyms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ayush\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'hyponyms' is not defined"],"ename":"NameError","evalue":"name 'hyponyms' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# shuffled_vocab = copy.deepcopy(vocab)\n\ndef find_negative_samples(x):\n    answer = []\n    while len(answer)<neg_sample_count:\n        hm = random.choice(vocab)\n        if hm in hyponym_hypernyms[x] and hm not in vocab:\n            continue\n        else:\n            answer.append(word2index[hm])\n    \n    return answer","metadata":{"execution":{"iopub.status.busy":"2023-05-03T10:50:42.130942Z","iopub.execute_input":"2023-05-03T10:50:42.131491Z","iopub.status.idle":"2023-05-03T10:50:42.139667Z","shell.execute_reply.started":"2023-05-03T10:50:42.131446Z","shell.execute_reply":"2023-05-03T10:50:42.138656Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"embedding_size = dim\nmodel = HHD(vocab_size,embedding_size)\nmodel.to(device)\n# model.to(device)   \n\ncriterion = nn.BCEWithLogitsLoss(weight=None, reduction=\"sum\")\noptimizer = optim.Adam(model.parameters(), lr=lr)","metadata":{"execution":{"iopub.status.busy":"2023-05-03T10:50:45.399513Z","iopub.execute_input":"2023-05-03T10:50:45.400259Z","iopub.status.idle":"2023-05-03T10:50:45.632671Z","shell.execute_reply.started":"2023-05-03T10:50:45.400219Z","shell.execute_reply":"2023-05-03T10:50:45.631625Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"def train(model,data,gold):\n    data_size = len(data)\n    queries = []\n    hypernyms = []\n    neg_samples = []\n    running_loss = []\n    rows = 0\n\n\n    for i, query in tqdm(enumerate(data)):\n        for j, hypernym in enumerate(gold[i]):\n            try:\n                q = word2index[query]\n                h = word2index[hypernym]\n            except:\n    #             print(query,hypernym)\n                continue\n\n            # add query to list\n            queries.append(q) #255 - batch size\n            # add hypermym to list\n            hypernyms.append(h) #255\n            # add negative samples to list\n            neg_samples.append(find_negative_samples(query)) # 255*5\n\n            rows += 1\n            if rows % batch_size == 0:\n\n                # make tensor from query list\n                queries_t = torch.tensor(queries, dtype=torch.long).to(device) #255\n                \n                # make tensor from hypernym list\n                hypernyms_t = torch.tensor(hypernyms, dtype=torch.long).to(device) #255\n                # make tensor form negative sampleS list\n                neg_samples_t = torch.tensor(neg_samples, dtype=torch.long).to(device) #255*5\n\n\n                # pass to model\n                optimizer.zero_grad()\n                simPos,simNegs = model(queries_t, hypernyms_t, neg_samples_t) #255*1 , 255*5\n\n    #             output = torch.cat((simPos,simNegs),1) #255*6\n    #             print(\"256*\")\n\n                y_pos = torch.ones((simPos.shape[0],1)).to(device) #255*1\n                y_neg = torch.zeros((simNegs.shape[0],neg_sample_count)).to(device) #255*5\n    #             target = torch.cat((y_pos,y_neg),1) #255*6\n\n                # calculate positive and negative loss\n                pos_loss = criterion(simPos, y_pos)\n                neg_loss = criterion(simNegs, y_neg)\n                loss = neg_loss + pos_loss\n\n                # back propogate the loss\n                loss.backward()\n                optimizer.step()\n                running_loss.append(loss.item())\n                # clear the lists\n\n                del queries_t\n                del hypernyms_t\n                del neg_samples_t\n\n                queries.clear()\n                hypernyms.clear()\n                neg_samples.clear()\n\n    epoch_loss = np.mean(running_loss)\n    print(\"Training epoch_loss is\", epoch_loss)\n    return epoch_loss","metadata":{"execution":{"iopub.status.busy":"2023-05-03T10:50:48.279531Z","iopub.execute_input":"2023-05-03T10:50:48.280244Z","iopub.status.idle":"2023-05-03T10:50:48.291480Z","shell.execute_reply.started":"2023-05-03T10:50:48.280205Z","shell.execute_reply":"2023-05-03T10:50:48.290200Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"for i in range(10):\n    train(model,data,gold)","metadata":{"execution":{"iopub.status.busy":"2023-05-03T10:51:15.162379Z","iopub.execute_input":"2023-05-03T10:51:15.163076Z","iopub.status.idle":"2023-05-03T10:51:20.150455Z","shell.execute_reply.started":"2023-05-03T10:51:15.163036Z","shell.execute_reply":"2023-05-03T10:51:20.149363Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stderr","text":"500it [00:00, 968.58it/s] \n","output_type":"stream"},{"name":"stdout","text":"Training epoch_loss is 27.47876179471929\n","output_type":"stream"},{"name":"stderr","text":"500it [00:00, 1032.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training epoch_loss is 26.28834651378875\n","output_type":"stream"},{"name":"stderr","text":"500it [00:00, 1031.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training epoch_loss is 24.77139783412852\n","output_type":"stream"},{"name":"stderr","text":"500it [00:00, 987.66it/s] \n","output_type":"stream"},{"name":"stdout","text":"Training epoch_loss is 24.283480096370617\n","output_type":"stream"},{"name":"stderr","text":"500it [00:00, 984.77it/s] \n","output_type":"stream"},{"name":"stdout","text":"Training epoch_loss is 22.471427339188597\n","output_type":"stream"},{"name":"stderr","text":"500it [00:00, 1031.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training epoch_loss is 19.226755903122275\n","output_type":"stream"},{"name":"stderr","text":"500it [00:00, 1026.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training epoch_loss is 16.602910001227198\n","output_type":"stream"},{"name":"stderr","text":"500it [00:00, 1034.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training epoch_loss is 14.579110683278834\n","output_type":"stream"},{"name":"stderr","text":"500it [00:00, 1033.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training epoch_loss is 13.423619818180166\n","output_type":"stream"},{"name":"stderr","text":"500it [00:00, 1020.96it/s]","output_type":"stream"},{"name":"stdout","text":"Training epoch_loss is 12.562704380522383\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"def evaluate(model,data,gold):\n    \n    model.eval()\n    \n    data_size = len(data)\n    queries = []\n    hypernyms = []\n    neg_samples = []\n    running_loss = []\n    rows = 0\n\n\n    for i, query in tqdm(enumerate(data)):\n        for j, hypernym in enumerate(gold[i]):\n            try:\n                q = word2index[query]\n                h = word2index[hypernym]\n            except:\n    #             print(query,hypernym)\n                continue\n\n            # add query to list\n            queries.append(q) #255 - batch size\n            # add hypermym to list\n            hypernyms.append(h) #255\n            # add negative samples to list\n            neg_samples.append(find_negative_samples(query)) # 255*5\n\n            rows += 1\n            if rows % batch_size == 0:\n\n                # make tensor from query list\n                queries_t = torch.tensor(queries, dtype=torch.long).to(device) #255\n                \n                # make tensor from hypernym list\n                hypernyms_t = torch.tensor(hypernyms, dtype=torch.long).to(device) #255\n                # make tensor form negative sampleS list\n                neg_samples_t = torch.tensor(neg_samples, dtype=torch.long).to(device) #255*5\n\n\n                # pass to model\n#                 optimizer.zero_grad()\n                simPos,simNegs = model(queries_t, hypernyms_t, neg_samples_t) #255*1 , 255*5\n\n    #             output = torch.cat((simPos,simNegs),1) #255*6\n    #             print(\"256*\")\n\n                y_pos = torch.ones((simPos.shape[0],1)).to(device) #255*1\n                y_neg = torch.zeros((simNegs.shape[0],neg_sample_count)).to(device) #255*5\n    #             target = torch.cat((y_pos,y_neg),1) #255*6\n\n                # calculate positive and negative loss\n                pos_loss = criterion(simPos, y_pos)\n                neg_loss = criterion(simNegs, y_neg)\n                loss = neg_loss + pos_loss\n\n                # back propogate the loss\n#                 loss.backward()\n#                 optimizer.step()\n                running_loss.append(loss.item())\n                # clear the lists\n\n                del queries_t\n                del hypernyms_t\n                del neg_samples_t\n\n                queries.clear()\n                hypernyms.clear()\n                neg_samples.clear()\n\n    epoch_loss = np.mean(running_loss)\n    print(\"Training epoch_loss is\", epoch_loss)\n    return epoch_loss","metadata":{"execution":{"iopub.status.busy":"2023-05-03T10:51:23.314067Z","iopub.execute_input":"2023-05-03T10:51:23.314444Z","iopub.status.idle":"2023-05-03T10:51:23.327195Z","shell.execute_reply.started":"2023-05-03T10:51:23.314410Z","shell.execute_reply":"2023-05-03T10:51:23.326001Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"evaluate(model,data,gold)","metadata":{"execution":{"iopub.status.busy":"2023-05-03T10:51:24.116279Z","iopub.execute_input":"2023-05-03T10:51:24.116698Z","iopub.status.idle":"2023-05-03T10:51:24.261344Z","shell.execute_reply.started":"2023-05-03T10:51:24.116659Z","shell.execute_reply":"2023-05-03T10:51:24.260299Z"},"trusted":true},"execution_count":86,"outputs":[{"name":"stderr","text":"500it [00:00, 3785.68it/s]","output_type":"stream"},{"name":"stdout","text":"Training epoch_loss is 11.670052903763791\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"11.670052903763791"},"metadata":{}}]},{"cell_type":"code","source":"def predict(query):\n    data_size = len(data)\n    queries = []\n    hypernyms = []\n    neg_samples = []\n    running_loss = []\n    rows = 0\n    try:\n        q = torch.tensor([word2index[query]]).to(device)\n    except:\n        return \"word not found in vocab\"\n    closest_hypernyms = [] #[[similarity,word]]\n#     for i,cand_hypernym in tqdm(enumerate(vocab[:-2])):\n#         if query == cand_hypernym:\n#             continue\n        \n#         try:\n#             h = torch.tensor([word2index[cand_hypernym]])\n#         except:\n#             continue\n        \n#         s = model.similarity(q,h)\n        \n#         closest_hypernyms.append([s,cand_hypernym])\n    \n    h = torch.tensor(list(range(1,vocab_size))).to(device)\n    s = model.similarity(q,h,h.shape[0]) #bs*1\n#     to do - append similarities for all words\n    for i in range(1,vocab_size):\n        closest_hypernyms.append([float(s[i-1]),vocab[i]])\n    closest_hypernyms.sort(reverse=True)\n    answer = []\n    \n    l = 100\n    if l>len(closest_hypernyms):\n        l = len(closest_hypernyms)\n    \n    for i in range(l):\n        answer.append(closest_hypernyms[i][1])\n        \n    return answer","metadata":{"execution":{"iopub.status.busy":"2023-05-03T10:51:30.687532Z","iopub.execute_input":"2023-05-03T10:51:30.688246Z","iopub.status.idle":"2023-05-03T10:51:30.697503Z","shell.execute_reply.started":"2023-05-03T10:51:30.688208Z","shell.execute_reply":"2023-05-03T10:51:30.696459Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"predict(\"tropical_storm\")","metadata":{"execution":{"iopub.status.busy":"2023-05-03T10:51:32.897625Z","iopub.execute_input":"2023-05-03T10:51:32.897993Z","iopub.status.idle":"2023-05-03T10:51:32.905200Z","shell.execute_reply.started":"2023-05-03T10:51:32.897959Z","shell.execute_reply":"2023-05-03T10:51:32.904073Z"},"trusted":true},"execution_count":88,"outputs":[{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"'word not found in vocab'"},"metadata":{}}]},{"cell_type":"code","source":"torch.save(model, '/kaggle/working/HH_Projection_model_2B.pt')","metadata":{"execution":{"iopub.status.busy":"2023-05-03T10:53:32.042503Z","iopub.execute_input":"2023-05-03T10:53:32.043218Z","iopub.status.idle":"2023-05-03T10:53:32.297314Z","shell.execute_reply.started":"2023-05-03T10:53:32.043178Z","shell.execute_reply":"2023-05-03T10:53:32.296141Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"train(model,data,gold)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate(model,data,gold)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(10):\n    train(model,data,gold)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# playing","metadata":{}},{"cell_type":"code","source":"var = 2 / (dim + dim)\n\nproj_mats = torch.zeros([p, dim, dim], dtype=torch.float32)\n\n# Fills self tensor with elements samples from the normal distribution parameterized by mean and std.\nproj_mats.normal_(0, var)\n\n\n# mat_data is of size k*dim*dim\nproj_mats += torch.cat([torch.eye(dim, ).unsqueeze(0) for _ in range(p)])\n\n# finally mat_data is k*dim*dim matrix ie k projection matrices\n# each matric is populated with random value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(vocab[:20])\nprint(data[:20])\nprint(gold[:20])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = torch.randn(255)\ny = torch.randn(255)\nz=  torch.cat((x,y))\n# print(x[:4])\n# print(y[:4])\nprint(z.shape)\nz = z.reshape([2,5,-1])\nprint(z.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-02T17:25:55.572714Z","iopub.execute_input":"2023-05-02T17:25:55.573078Z","iopub.status.idle":"2023-05-02T17:25:55.580530Z","shell.execute_reply.started":"2023-05-02T17:25:55.573048Z","shell.execute_reply":"2023-05-02T17:25:55.579340Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"torch.Size([510])\ntorch.Size([2, 5, 51])\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}